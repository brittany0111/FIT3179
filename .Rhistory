?dibinom
?dbinom
print(P)
print(p)
p = 1-pbinom(4,10,1/2)
print(p)
p = 1 - pnorm(2, 0, 1)
print(p)
p = 1 - pnorm(2, 0, 4)
print(p)
exp(-4)
4*exp(-4)
exp(-4)
((4^2)*exp(-4))/factorial(2)
ppois(1,4)
ppois(2,4)
?ppois
dpois(1,4)
dpois(2,4)
4^1(exp(-4))/factorial(1)
4*(exp-4)/factorial(1)
geometric_pmf <- function(y, phi) {
exp(phi) / (exp(phi) + 1)^(y + 1)
}
# Values of y from 0 to 20
y_values <- 0:20
# Define values of phi
phi_values <- c(0, 1, 2)
# Calculate PMF for each phi
pmf_phi_0 <- geometric_pmf(y_values, phi_values[1])
pmf_phi_1 <- geometric_pmf(y_values, phi_values[2])
pmf_phi_2 <- geometric_pmf(y_values, phi_values[3])
# Plot the PMF
plot(y_values, pmf_phi_0, type="o", col="blue", ylim=c(0, 1), xlab="y (Number of failures)",
ylab="P(y | ϕ)", main="Geometric Probability Mass Function", lwd=2)
lines(y_values, pmf_phi_1, type="o", col="red", lwd=2)
lines(y_values, pmf_phi_2, type="o", col="green", lwd=2)
# Add a legend
legend("topright", legend=c("ϕ = 0", "ϕ = 1", "ϕ = 2"), col=c("blue", "red", "green"), lty=1, lwd=2)
# Add grid
grid()
geometric_pmf <- function(y, phi) {
exp(phi) / (exp(phi) + 1)^(y + 1)
}
# Values of y from 0 to 20
y_values <- 0:20
# Define values of phi
phi_values <- c(0, 1, 2)
# Calculate PMF for each phi
pmf_phi_0 <- geometric_pmf(y_values, phi_values[1])
pmf_phi_1 <- geometric_pmf(y_values, phi_values[2])
pmf_phi_2 <- geometric_pmf(y_values, phi_values[3])
# Plot the PMF
plot(y_values, pmf_phi_0, type="o", col="blue", ylim=c(0, 1), xlab="y (Number of failures)",
ylab="P(y | ϕ)", main="Geometric Probability Mass Function", lwd=2)
lines(y_values, pmf_phi_1, type="o", col="red", lwd=2)
lines(y_values, pmf_phi_2, type="o", col="green", lwd=2)
# Add a legend
legend("topright", legend=c("ϕ = 0", "ϕ = 1", "ϕ = 2"), col=c("blue", "red", "green"), lty=1,
geometric_pmf <- function(y, phi) {
exp(phi) / (exp(phi) + 1)^(y + 1)
}
# Values of y from 0 to 20
y_values <- 0:20
# Plot the PMF
plot(y_values, pmf_phi_0, type="o", col="blue", ylim=c(0, 1), xlab="y (Number of failures)",
ylab="P(y | ϕ)", main="Geometric Probability Mass Function", lwd=2)
lines(y_values, pmf_phi_1, type="o", col="red", lwd=2)
lines(y_values, pmf_phi_2, type="o", col="green", lwd=2)
# Add a legend
legend("topright", legend=c("ϕ = 0", "ϕ = 1", "ϕ = 2"), col=c("blue", "red", "green"), lty=1, lwd=2)
# Plot the PMF
plot(y_values, pmf_phi_0, type="o", col="blue", ylim=c(0, 1), xlab="y (number of failures = seeing a tail)",
ylab="P(y | ϕ)", main="Geometric Probability Mass Function", lwd=2)
lines(y_values, pmf_phi_1, type="o", col="red", lwd=2)
lines(y_values, pmf_phi_2, type="o", col="green", lwd=2)
# Add a legend
legend("topright", legend=c("ϕ = 0", "ϕ = 1", "ϕ = 2"), col=c("blue", "red", "green"), lty=1, lwd=2)
# Plot the PMF
plot(y_values, pmf_phi_0, type="o", col="red", ylim=c(0, 1), xlab="y (number of failures = seeing a tail)",
ylab="P(y | ϕ)", main="Geometric Probability Mass Function", lwd=2)
lines(y_values, pmf_phi_1, type="o", col="pink", lwd=2)
lines(y_values, pmf_phi_2, type="o", col="green", lwd=2)
# Add a legend
legend("topright", legend=c("ϕ = 0", "ϕ = 1", "ϕ = 2"), col=c("red", "pink", "green"), lty=1, lwd=2)
# Add a legend
legend("topright", legend=c("ϕ = 0", "ϕ = 1", "ϕ = 2"), col=c("red", "pink", "blue"), lty=1, lwd=2)
# Plot the PMF
plot(y_values, pmf_phi_0, type="o", col="orange", ylim=c(0, 1), xlab="y (number of failures = seeing a tail)",
ylab="P(y | ϕ)", main="Geometric Probability Mass Function", lwd=2)
lines(y_values, pmf_phi_1, type="o", col="pink", lwd=2)
lines(y_values, pmf_phi_2, type="o", col="blue", lwd=2)
# Add a legend
legend("topright", legend=c("ϕ = 0", "ϕ = 1", "ϕ = 2"), col=c("orange", "pink", "blue"), lty=1, lwd=2)
binom.test(x = 168, n = 240, p = 0.5, alternative = "greater")
setwd('/Users/Downloads')
getwd()
setwd('Users/brittanylau/Downloads')
Dec21PedCount = read.csv("Ped_Count_December_2021.csv", header = TRUE)
str(Dec21PedCount)
dim(Dec21PedCount)
Dec21PedCount = lapply(Dec21PedCount[3:83], as.numeric) # applies function to columns in a df and returns as list, iapply returns output as vector / array
class(Dec21PedCount)
Dec21PedCount = as.data.frame(Dec21PedCount) # change class to df
# Q4
colMeans(Dec21PedCount, na.rm = TRUE) # na.rm() removes null values
# Q4
round(colMeans(Dec21PedCount, na.rm = TRUE), 2)
# Q5
boxplot(Dec21PedCount)
# Q5
boxplot(Dec21PedCount, las = 2)
# Q5
par(mar = c(15,4,4,2))
boxplot(Dec21PedCount, las = 2)
# Q5
par(mar = c(18,4,4,2))
boxplot(Dec21PedCount, las = 2)
# Q5
par(mar = c(18,4,4,2))
boxplot(Dec21PedCount, las = 2)
# Q5
par(mar = c(8,4,4,2))
boxplot(Dec21PedCount, las = 2)
# Q5
par(mar = c(18,4,4,2))
boxplot(Dec21PedCount, las = 2)
# Q6
ts(Dec21PedCount)
# Q6
MC.TS = ts(Dec21PedCount$Melbourne.Central, frequency = 24, start = c(2021,1))
plot(MC.TS)
# Q5
par(mar = c(15,4,4,2))
boxplot(Dec21PedCount, las = 2)
# Q6
MC.TS = ts(Dec21PedCount$Melbourne.Central, frequency = 24, start = c(2021,1))
plot(MC.TS)
plot(decomp)
decomp = decompose(MC.TS)
plot(decomp)
# a)
summary(pacific)
# Q5
pacific = c(209, 48, 169, 138, 64, 97, 161, 95, 145, 90, 121, 80, 56, 64, 209, 64, 72, 288, 322)
tasman = c(76, 64, 68, 64, 37, 32, 32, 51, 56, 40, 64, 56, 80, 121, 177, 56, 80, 35, 72, 72, 108, 48)
# a)
summary(pacific)
summary(tasman)
boxplot(pacific, las = 2)
boxplot(tasman, las = 2)
# b) Test the hypothesis that rivers flowing into the Tasman Sea are shorter on average than
#    those flowing into the Pacific Ocean. Use a significance of 1%
t.test(pacific, tasman, alternative = 'greater')
help(t.test)
boxplot(pacific, las = 2)
t.test(tasman, pacific, alternative = 'less', conf.level = 0.99) # gives same answer
t.test(pacific, tasman, alternative = 'greater', conf.level = 0.99) # starts with pacific due to our alt. hypothesis
help(plot)
help(scatter)
??scatter
metacarp = c(45,51,39,41,48,49,46,43,47)
stature = c(171,178,157,163,172,183,173,175,173)
plot(metacarp,stature)
fit = lm(stature ~.,metacarp)
help(lm)
fit = lm(stature ~ metacarp)
fit
help(abline)
abline(fit)
fitted = lm(stature ~ metacarp) # Calculate regression equation
abline(fitted) # Line of best fit
summary(fitted)
abline(fitted, col = 'red') # Line of best fit
kiama = read.table('kiama.txt', header = TRUE)
kiama
kiama = as.data.frame(kiama)
kiama
kiama_df = as.data.frame(kiama)
kiama_df
kiama_vector = iapply(kiama_df[:]. as.numeric)
kiama_vector = iapply(kiama_df[::]. as.numeric)
kiama_vector = iapply(kiama_df[1:64]. as.numeric)
kiama_vector = iapply(kiama_df[1:64], as.numeric)
kiama = read.table('kiama.txt', header = TRUE)
kiama_vector = as.vector(t(as.matrix(kiama[-1])))
kiama_vector
kiama_vector = as.vector(kiama)
kiama = read.table('kiama.txt', header = TRUE)
summary(kiama)
sd(kiama$Interval)
hist(kiama)
hist(kiama$Interval)
hist(kiama$Interval, main = 'Histogram of Kiama Eruptions', xlab = 'Intervals') # Shows frequency of values for each RANGE of values
hist(kiama$Interval, col = 'blue', main = 'Histogram of Kiama Eruptions', xlab = 'Intervals') # Shows frequency of values for each RANGE of values
hist(kiama$Interval, col = c('blue','pink','yellow''), main = 'Histogram of Kiama Eruptions', xlab = 'Intervals') # Shows frequency of values for each RANGE of values
hist(kiama$Interval, col = c('blue','pink','yellow'), main = 'Histogram of Kiama Eruptions', xlab = 'Intervals') # Shows frequency of values for each RANGE of values
hist(kiama$Interval, col = c('skyblue','pink','yellow'), main = 'Histogram of Kiama Eruptions', xlab = 'Intervals') # Shows frequency of values for each RANGE of values
timber = read.table('timber.txt', header = TRUE)
df = as.data.frame(timber)
df
# cor() - ranges from -1 to 1, finds out which predictor is better at predicting rigidity
cor(df$Elast,df$Dens)
help(cor)
timber = read.table('timber.txt', header = TRUE)
timber = as.data.frame(timber)
timber
# cor() - ranges from -1 to 1, finds out which predictor is better at predicting rigidity
cor(timber$Elast,timber$Rigid)
cor(timber$Dens,timber$Rigid)
# cor() - ranges from -1 to 1, finds out which predictor is better at predicting rigidity
cor(timber)
# b)
fitted = lm(timber$Dens ~ timber)
fitted = lm(timber$Rigid ~ timber$Dens)
fitted
plot(timber$Rigid, timber$Dens)
summary(fitted)
abline(fitted, col='red')
plot(timber$Dens, timber$Rigid)
abline(fitted, col='red')
plot(timber$Dens, timber$Rigid, xlab = "Density", ylab = "Rigidity") # in scatter plot, x-axis always predictor, y-axis is target
abline(fitted, col='red')
# c)
fitted = lm(timber$Rigid ~ timber$Dens + timber$Elast) # Uses both density and elasticity
fitted
summary(fitted)
summary(fitted)
investA = read.csv("InvestA.csv")
investA
uniq(investA$Group)
unique(investA$Group)
# Boxplot for EACH of the 6 groups
boxplot(investA$FV~ investA$Group)
help("aggregate")
x = aggregate(investA$FV, list(investA$Group, "mean"))
investA = as.data.frame(investA)
unique(investA$Group) # 6 unique groups
# Boxplot for EACH of the 6 groups
boxplot(investA$FV ~ investA$Group)
x = aggregate(investA$FV, list(investA$Group, "mean"))
x = aggregate(investA$FV, list(investA$Group), "mean"))
x = aggregate(investA$FV, list(investA$Group), "mean")
x
class(x)
y = tapply(investA$FV. list(investA$Group), "mean")
y = tapply(investA$FV, list(investA$Group), "mean")
y
class(y) # ARRAY
z = by(investA$FV, investA$Group, mean)
z
class(z)
retail = read.csv("Q10.csv")
head(retail)
clo.TS = ts(retail[343:474,4], frequency = 12, start = c(2010,1)) # Start on January 2010
plot(clo.TS)
decomp = decompose(clo.TS)
# Clothing
clo.TS = ts(as.numeric(retail[343:474,4]), frequency = 12, start = c(2010,1)) # Start on January 2010
plot(clo.TS)
decomp = decompose(clo.TS)
plot(decomp)
dpt.TS = ts(as.numeric(retail[343:474,5]), frequency = 12, start = c(2010,1)) # Start on January 2010
plot(dpt.TS)
dpt_decomp = decompose(dpt.TS)
plot(dpt_decomp)
caf.TS = ts(as.numeric(retail[343:474,7]), frequency = 12, start = c(2010,1)) # Start on January 2010
plot(caf.TS)
caf_decomp = decompose(caf.TS)
plot(caf_decomp)
split(table$Country, table$Cluster) # Separate countries into their clusters
rect.hclust(hc, k = 10, border = "red")
# Pre-processing
table = read.csv("clustering_table.csv")
write.csv(top_100_adv, "Top100_Customers.csv", row.names = FALSE)
library(dplyr)
# Read data
df <- read.csv("Data_Analyst_Intern_Prework_Dataset.csv")
beverages <- df %>%
filter(Category %in% c("SUGARY DRINKS", "CULINARY"))
# Step 2: Aggregate by customer
customer_summary_adv <- beverages %>%
group_by(Customer.Id, Customer.Name) %>%
summarise(
total_beverage_spend = sum(Sales.Amount, na.rm = TRUE),
num_orders           = n_distinct(Order.ID),
num_brands           = n_distinct(Brand)  # how many brands they already stock
) %>%
ungroup()
# Step 3: Normalize values (so they’re comparable)
customer_summary_adv <- customer_summary_adv %>%
mutate(
spend_norm   = total_beverage_spend / max(total_beverage_spend),
orders_norm  = num_orders / max(num_orders),
brands_norm  = num_brands / max(num_brands),
# Weighted score
coca_cola_score = (0.5 * spend_norm) + (0.3 * orders_norm) + (0.2 * brands_norm)
)
top_100_adv <- customer_summary_adv %>%
arrange(desc(coca_cola_score)) %>%
slice(1:100)
top_100_adv$Rank <- seq.int(nrow(top_100_adv))
top_100_adv <- top_100_adv[, c("Rank", setdiff(names(top_100_adv), "Rank"))]
write.csv(top_100_adv, "Top100_Customers.csv", row.names = FALSE)
setwd("/Users/brittanylau/FIT3179-1/W10")
df = read.csv("forest_reserve_state copy.csv")
# Remove rows where state == "Selangor"
df <- df %>% filter(state != "Selangor")
library(dplyr)
df = read.csv("forest_reserve_state copy.csv")
# Remove rows where state == "Selangor"
df <- df %>% filter(state != "Selangor")
df = read.csv("forest_reserve_state copy.csv")
df = read.csv("forest_reserve_state copy.csv")
df <- df %>% filter(state != "Semenanjung Malaysia")
df
write.csv(df_filtered, "forest_reserve_filtered.csv", row.names = FALSE)
write.csv(df, "forest_reserve_filtered.csv", row.names = FALSE)
df = read.csv("forest_reserve_state copy.csv")
df <- df %>% filter(state != "Semenanjung Malaysia")
df
na_rows <- df[!complete.cases(df), ]
na_rows
df <- na.omit(df)
na_rows <- df[!complete.cases(df), ]
na_rows
write.csv(df, "forest_reserve_filtered.csv", row.names = FALSE)
df <- df[complete.cases(df$area), ]
write.csv(df, "forest_reserve_filtered.csv", row.names = FALSE)
setwd("/Users/brittanylau/FIT3179-1")
# read your CSV
df = pd.read_csv("tree_cover_loss_by_driver.csv")
import pandas as pd
library(dplyr)
df = read.csv("forest_reserve_state_normalized.csv")
# Remove all rows with any NA values
df <- df %>%
drop_na()
df <- df[complete.cases(df), ]
write.csv(df, "forest_reserve_state_nomr_clean.csv", row.names = FALSE)
df <- read.csv("forest_reserve_state_normalized.csv") %>%
mutate(date = gsub("-01-01", "", date))
head(df)
write.csv(data, "forest_reserve_state_normalized_clean.csv", row.names = FALSE)
write.csv(df, "forest_reserve_state_normalized_clean.csv", row.names = FALSE)
